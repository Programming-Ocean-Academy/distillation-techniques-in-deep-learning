{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Knowledge Distillation (KD)\n",
        "\n",
        "## Definition and Core Idea\n",
        "\n",
        "**Knowledge Distillation (KD)** is a training paradigm where a smaller or cheaper **student model** learns to mimic a stronger **teacher model** (often a large model or an ensemble).  \n",
        "The central objective is to **transfer knowledge** efficiently from teacher to student—maintaining high accuracy while reducing computational cost.\n",
        "\n",
        "The foundational idea emerged from **model compression** (Buciluă, Caruana, Niculescu-Mizil, 2006) and was formalized by **Hinton et al. (2015)** through the use of **soft targets** and a **temperature-scaled softmax**.\n",
        "\n",
        "---\n",
        "\n",
        "## Why Soft Targets?\n",
        "\n",
        "Hard one-hot labels contain no information about class similarity.  \n",
        "Soft targets (from a high-temperature softmax) encode **“dark knowledge”**—latent information about how the teacher perceives similarities among classes.  \n",
        "This provides richer gradients, smoother optimization, and better generalization.\n",
        "\n",
        "---\n",
        "\n",
        "## Main Families of Knowledge Distillation\n",
        "\n",
        "### 1. Response / Logit-Based KD\n",
        "Match the teacher’s **logits** or **probabilities**:\n",
        "$$\n",
        "L_{\\text{KD}} = T^2 \\cdot \\text{KL}\\!\\left( p_T^{(T)} \\parallel p_S^{(T)} \\right)\n",
        "$$\n",
        "where  \n",
        "$$\n",
        "p_i^{(T)} = \\frac{\\exp(z_i / T)}{\\sum_j \\exp(z_j / T)} .\n",
        "$$  \n",
        "This is the **canonical baseline**, widely used in CV and NLP.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Feature-Based KD\n",
        "Align intermediate **hidden activations** between teacher and student:\n",
        "$$\n",
        "L_{\\text{feat}} = \\sum_{k=1}^{K} \\| h_T^{(k)} - h_S^{(k)} \\|_2^2\n",
        "$$\n",
        "Used when architectures differ or when mid-level representations are crucial.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Relation-Based KD\n",
        "Match **relations** among samples or channels (e.g., pairwise distances or angles):\n",
        "$$\n",
        "L_{\\text{rel}} = \\sum_{i,j} \\big( d_T(x_i, x_j) - d_S(x_i, x_j) \\big)^2 .\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Attention-Based KD\n",
        "Force the student to reproduce the teacher’s **attention maps**:\n",
        "$$\n",
        "L_{\\text{attn}} = \\| A_T - A_S \\|_2^2 .\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### 5. Information-Theoretic KD\n",
        "Maximize the **mutual information** between teacher and student representations:\n",
        "$$\n",
        "\\max I(h_T; h_S) .\n",
        "$$\n",
        "Variational Information Distillation (VID) provides an estimation bound for this quantity.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. Sequence-Level KD (Seq2Seq)\n",
        "Distill **entire output sequences** instead of token-wise targets.  \n",
        "Simplifies decoding in NMT and often removes the need for beam search.\n",
        "\n",
        "---\n",
        "\n",
        "### 7. Self-Distillation / Online & Mutual\n",
        "The model teaches itself:\n",
        "- *Born-Again Networks*: each generation serves as a teacher for the next.\n",
        "- *Deep Mutual Learning*: multiple students learn cooperatively.\n",
        "\n",
        "---\n",
        "\n",
        "### 8. Data-Free / Privacy-Preserving KD\n",
        "Distill knowledge **without access to original data**, using synthetic samples or inversion (e.g., *Zero-Shot KD*, *DeepInversion*).  \n",
        "Useful for **federated** or **privacy-sensitive** contexts.\n",
        "\n",
        "---\n",
        "\n",
        "### 9. Diffusion-Model Distillation\n",
        "Compress diffusion sampling or enable controllable generation:\n",
        "- **Progressive Distillation:** halve steps repeatedly (e.g., 8192 → 4).\n",
        "- **Consistency Models:** single-step generation distilled from diffusion backbones.\n",
        "- **Score-Distillation Sampling (SDS):**\n",
        "  $$ \\nabla_\\theta L = \\mathbb{E}_x [ \\| s_\\theta(x) - s_T(x) \\|^2 ] $$\n",
        "  where \\( s_T \\) is the teacher score.\n",
        "- **Adversarial Diffusion Distillation (ADD):** combines diffusion and GAN objectives.\n",
        "\n",
        "---\n",
        "\n",
        "### 10. LLM-Focused KD\n",
        "Distillation for **Transformers and LLMs**:\n",
        "- *DistilBERT*, *TinyBERT*, *MiniLM(v2)* — combine logit, hidden-state, and attention-relation objectives.\n",
        "- Achieve up to **90 % reduction** in cost while maintaining strong GLUE performance.\n",
        "\n",
        "---\n",
        "\n",
        "## Applications and Goals\n",
        "\n",
        "1. **Compression & Acceleration:** shrink model size and latency while preserving accuracy.  \n",
        "2. **Better Training Signals:** improve generalization in low-data or noisy-label scenarios.  \n",
        "3. **Heterogeneous / Federated Learning:** share only predictions, not parameters.  \n",
        "4. **Diffusion & 3D:** compress multi-step sampling into one or few steps.\n",
        "\n",
        "---\n",
        "\n",
        "## How KD Works (Conceptual Model)\n",
        "\n",
        "**Soft-target alignment:**\n",
        "$$\n",
        "L = (1 - \\alpha)\\,\\text{CE}(y, p_S)\n",
        "    + \\alpha\\,T^2\\,\\text{KL}\\!\\left( p_T^{(T)} \\parallel p_S^{(T)} \\right)\n",
        "$$\n",
        "- Higher \\( T \\) reveals inter-class structure.\n",
        "- \\( \\alpha \\) balances hard vs. soft supervision.\n",
        "\n",
        "**Feature/attention/relational losses** regularize the internal geometry of representations, enhancing robustness to architecture mismatch.\n",
        "\n",
        "**Information-theoretic view:** maximize \\( I(h_T; h_S) \\).  \n",
        "**Sequence-level view:** approximate teacher decoding distributions to simplify generation.\n",
        "\n",
        "---\n",
        "\n",
        "## Practical Recipes\n",
        "\n",
        "**Baseline Logit KD**\n",
        "$$\n",
        "L = \\text{CE}(y, p_S) + \\lambda\\,T^2\\,\\text{KL}\\!\\left( p_T^{(T)} \\parallel p_S^{(T)} \\right)\n",
        "$$\n",
        "Typical ranges: \\( T \\in [2,8] \\), \\( \\lambda \\in [0.5,2] \\).\n",
        "\n",
        "**Intermediate Guidance (Feature KD):**  \n",
        "Match \\( K \\) layers using MSE or cosine similarity (“Patient KD”).\n",
        "\n",
        "**Attention/Relation Losses:**  \n",
        "Add AT/RKD/CRD terms when architectures differ.\n",
        "\n",
        "**For LLMs:**  \n",
        "Combine LM loss + KD loss + attention/hidden-state alignment at both pre-training and task stages.\n",
        "\n",
        "**For Diffusion Models:**  \n",
        "Use progressive or consistency distillation; SDS for cross-space transfer (e.g., NeRF).\n",
        "\n",
        "**For Federated Settings:**  \n",
        "FedMD: share logits on public data; fit local students to ensemble consensus.\n",
        "\n",
        "---\n",
        "\n",
        "## Tuning Tips & Pitfalls\n",
        "\n",
        "- **Temperature \\(T\\)** and **weight \\(\\lambda\\)** are crucial.  \n",
        "  Too low \\(T\\): little dark knowledge; too high \\(T\\): overly flat distributions.\n",
        "- **Teacher quality vs. student capacity:** small students may underfit; feature-based KD helps.\n",
        "- **Tokenizer mismatch (LLMs):** rely on attention/hidden-state KD.\n",
        "- **Data-free KD:** quality of synthetic data matters; expect some accuracy gap.\n",
        "- **Seq-level KD:** balance with a small hard-label loss to avoid over-imitation.\n",
        "- **Diffusion KD:** ensure correct supervision of score dynamics; beware gradient bias in SDS.\n",
        "\n",
        "---\n",
        "\n",
        "## When to Use KD\n",
        "\n",
        "Use KD when:\n",
        "- You need **smaller/faster/cheaper** models.\n",
        "- You are deploying on **edge devices**.\n",
        "- You want to **consolidate ensembles**.\n",
        "- You are aligning **different architectures or modalities**.\n",
        "- You aim to **accelerate diffusion or generative sampling**.\n",
        "\n",
        "Avoid or combine with alternatives when:\n",
        "- Bottleneck is I/O or CPU rather than FLOPs.\n",
        "- Teacher is not substantially better than student.\n",
        "- Pruning or quantization alone achieves the same gains.\n",
        "\n",
        "---\n",
        "\n",
        "## Minimal Starter KD Recipe\n",
        "\n",
        "**Setup**\n",
        "- Teacher: larger, well-trained model.  \n",
        "- Student: thinner or shallower model from the same family.\n",
        "\n",
        "**Loss**\n",
        "$$\n",
        "L = \\text{CE}(y, s)\n",
        "  + \\lambda\\,T^2\\,\\text{KL}\\!\\left( p_T^{(T)} \\parallel p_S^{(T)} \\right)\n",
        "  + \\alpha \\sum_{k=1}^{K} \\| h_T^{(k)} - h_S^{(k)} \\|_2^2\n",
        "$$\n",
        "\n",
        "**Typical Hyperparameters**\n",
        "- \\( T \\in [2,6] \\)\n",
        "- \\( \\lambda \\in [0.5,2] \\)\n",
        "- \\( \\alpha \\in [0.1,0.5] \\)\n",
        "\n",
        "**Evaluation**\n",
        "- Report accuracy vs. compute (latency, memory, energy).  \n",
        "- For diffusion: report FID / IS vs. number of sampling steps.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "mb_eLEAhhwH1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Foundational and Recent Papers on Knowledge Distillation\n",
        "\n",
        "## Root / Foundational Papers\n",
        "\n",
        "**Distilling the Knowledge in a Neural Network**  \n",
        "*(Hinton, Vinyals & Dean, 2015)*  \n",
        "This seminal work introduced **knowledge distillation (KD)** — a process where a smaller *student* model learns from the softened output probabilities of a larger *teacher* model using a **temperature-scaled softmax**. This allows the student to capture “dark knowledge” about inter-class similarities that the teacher has learned.  \n",
        "Source: *arXiv*\n",
        "\n",
        "---\n",
        "\n",
        "**Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks**  \n",
        "*(Papernot et al., 2015)*  \n",
        "This paper introduced **defensive distillation**, showing that the distillation process can enhance **adversarial robustness** by smoothing the model’s decision surface, thus reducing sensitivity to small input perturbations.  \n",
        "Source: *arXiv*\n",
        "\n",
        "---\n",
        "\n",
        "**On the Efficacy of Knowledge Distillation**  \n",
        "*(Cho & Hariharan, ICCV 2019)*  \n",
        "An empirical study exploring *when and why* distillation succeeds or fails. The results show that teacher-student **capacity mismatch** and **architectural differences** significantly affect distillation quality and generalization.  \n",
        "Source: *openaccess.thecvf.com*\n",
        "\n",
        "---\n",
        "\n",
        "**Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning**  \n",
        "*(Allen-Zhu & Li, 2020)*  \n",
        "A theoretical exploration connecting ensembles, distillation, and self-distillation. The paper provides a formal understanding of how ensemble knowledge can be compressed into a single model and explains the benefits of iterative self-distillation.  \n",
        "Source: *arXiv*\n",
        "\n",
        "---\n",
        "\n",
        "**Like What You Like: Knowledge Distill via Neuron Selectivity Transfer**  \n",
        "*(Mocol, Zhang et al., ~2019)*  \n",
        "Proposes **feature-based distillation** using *neuron selectivity distributions* and **maximum mean discrepancy (MMD)** to align internal feature representations between teacher and student.  \n",
        "Source: *openreview.net*\n",
        "\n",
        "---\n",
        "\n",
        "## Recent / Newer Papers\n",
        "\n",
        "**What Knowledge Gets Distilled in Knowledge Distillation?**  \n",
        "*(NeurIPS 2023)*  \n",
        "Investigates *what specific knowledge* is actually transferred during distillation — analyzing whether the student learns structure, label relationships, or other latent representations beyond mere output matching.  \n",
        "Source: *proceedings.neurips.cc*\n",
        "\n",
        "---\n",
        "\n",
        "**A Comprehensive Survey on Knowledge Distillation**  \n",
        "*(Mansourian et al., 2025)*  \n",
        "A comprehensive modern review of KD covering **transformers, large language models (LLMs), and diffusion models**, categorizing methods and analyzing theoretical and practical advances.  \n",
        "Source: *arXiv*\n",
        "\n",
        "---\n",
        "\n",
        "**Knowledge Distillation Meets Self-Supervision**  \n",
        "*(ECCV 2020)*  \n",
        "Integrates **self-supervised learning (SSL)** signals with distillation, enabling students to benefit from unlabeled data and enhancing generalization, especially under limited or noisy labels.  \n",
        "Source: *ecva.net*\n",
        "\n",
        "---\n",
        "\n",
        "**Knowledge Distillation Meets Open-Set Semi-Supervised Learning**  \n",
        "*(2024)*  \n",
        "Extends KD to **open-set semi-supervised** settings, demonstrating that distillation can guide students in domains with partially labeled or unknown-class data.  \n",
        "Source: *SpringerLink*\n",
        "\n",
        "---\n",
        "\n",
        "**A Coded Knowledge Distillation Framework for Image Classification**  \n",
        "*(2024)*  \n",
        "Introduces a **coded representation** approach for KD, encoding teacher knowledge into compact transferable forms to improve student learning in image classification.  \n",
        "Source: *ScienceDirect*\n",
        "\n",
        "---\n",
        "\n",
        "## Additional Notes & Thematic Insights\n",
        "\n",
        "- **Surveys** such as *Knowledge Distillation: A Survey* (Gou et al., 2020) provide taxonomies of KD types (response-based, feature-based, relation-based) and discuss architecture compatibility.  \n",
        "  Source: *arXiv*\n",
        "\n",
        "- **Broader perspective:** KD extends beyond compression — it is also a framework for **knowledge adaptation**, **domain transfer**, and **cross-modal learning**.\n",
        "\n",
        "- **Forms of transferred knowledge:**\n",
        "  - **Logit-based:** Teacher soft outputs.\n",
        "  - **Feature-based:** Intermediate activations or attention maps.\n",
        "  - **Relation-based:** Structural or pairwise sample relationships.\n",
        "\n",
        "- **Practical considerations:**\n",
        "  - Teacher–student capacity ratio.\n",
        "  - Architecture mismatch.\n",
        "  - Data size and availability.\n",
        "  - Distillation temperature and loss weighting.\n",
        "  - Whether distillation improves **generalization** or merely **memorization**.\n",
        "\n",
        "- **Empirical challenges** noted by *Cho & Hariharan (2019)* emphasize that KD is not universally beneficial — effectiveness depends on *teacher quality*, *data overlap*, and *training dynamics*.\n",
        "\n",
        "---\n",
        "\n",
        "## Core Distillation Equation\n",
        "\n",
        "Let the teacher produce logits \\( z_T \\) and the student produce logits \\( z_S \\). The **softened softmax** with temperature \\( T \\) is:\n",
        "\n",
        "$$\n",
        "p_i^{(T)} = \\frac{\\exp(z_i / T)}{\\sum_j \\exp(z_j / T)}\n",
        "$$\n",
        "\n",
        "The **distillation loss** is typically a weighted combination of the Kullback–Leibler divergence between teacher and student outputs and the standard cross-entropy with ground truth labels:\n",
        "\n",
        "$$\n",
        "L = (1 - \\alpha) \\cdot \\text{CE}(y, p_S) + \\alpha \\cdot T^2 \\cdot \\text{KL}(p_T^{(T)} \\parallel p_S^{(T)})\n",
        "$$\n",
        "\n",
        "where:\n",
        "- \\( \\alpha \\) balances between hard and soft targets,  \n",
        "- \\( T \\) controls the smoothness of distributions,  \n",
        "- \\( p_T^{(T)} \\) and \\( p_S^{(T)} \\) denote teacher and student soft probabilities.\n",
        "\n",
        "This formulation captures the essence of how **dark knowledge** (latent inter-class structure) is transferred from teacher to student.\n"
      ],
      "metadata": {
        "id": "hFP7LGRKiDwj"
      }
    }
  ]
}