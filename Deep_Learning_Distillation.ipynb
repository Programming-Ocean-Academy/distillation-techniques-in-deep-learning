{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Knowledge Distillation (KD)\n",
        "\n",
        "## Definition and Core Idea\n",
        "\n",
        "**Knowledge Distillation (KD)** is a training paradigm where a smaller or cheaper **student model** learns to mimic a stronger **teacher model** (often a large model or an ensemble).  \n",
        "The central objective is to **transfer knowledge** efficiently from teacher to student—maintaining high accuracy while reducing computational cost.\n",
        "\n",
        "The foundational idea emerged from **model compression** (Buciluă, Caruana, Niculescu-Mizil, 2006) and was formalized by **Hinton et al. (2015)** through the use of **soft targets** and a **temperature-scaled softmax**.\n",
        "\n",
        "---\n",
        "\n",
        "## Why Soft Targets?\n",
        "\n",
        "Hard one-hot labels contain no information about class similarity.  \n",
        "Soft targets (from a high-temperature softmax) encode **“dark knowledge”**—latent information about how the teacher perceives similarities among classes.  \n",
        "This provides richer gradients, smoother optimization, and better generalization.\n",
        "\n",
        "---\n",
        "\n",
        "## Main Families of Knowledge Distillation\n",
        "\n",
        "### 1. Response / Logit-Based KD\n",
        "Match the teacher’s **logits** or **probabilities**:\n",
        "$$\n",
        "L_{\\text{KD}} = T^2 \\cdot \\text{KL}\\!\\left( p_T^{(T)} \\parallel p_S^{(T)} \\right)\n",
        "$$\n",
        "where  \n",
        "$$\n",
        "p_i^{(T)} = \\frac{\\exp(z_i / T)}{\\sum_j \\exp(z_j / T)} .\n",
        "$$  \n",
        "This is the **canonical baseline**, widely used in CV and NLP.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Feature-Based KD\n",
        "Align intermediate **hidden activations** between teacher and student:\n",
        "$$\n",
        "L_{\\text{feat}} = \\sum_{k=1}^{K} \\| h_T^{(k)} - h_S^{(k)} \\|_2^2\n",
        "$$\n",
        "Used when architectures differ or when mid-level representations are crucial.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Relation-Based KD\n",
        "Match **relations** among samples or channels (e.g., pairwise distances or angles):\n",
        "$$\n",
        "L_{\\text{rel}} = \\sum_{i,j} \\big( d_T(x_i, x_j) - d_S(x_i, x_j) \\big)^2 .\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Attention-Based KD\n",
        "Force the student to reproduce the teacher’s **attention maps**:\n",
        "$$\n",
        "L_{\\text{attn}} = \\| A_T - A_S \\|_2^2 .\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### 5. Information-Theoretic KD\n",
        "Maximize the **mutual information** between teacher and student representations:\n",
        "$$\n",
        "\\max I(h_T; h_S) .\n",
        "$$\n",
        "Variational Information Distillation (VID) provides an estimation bound for this quantity.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. Sequence-Level KD (Seq2Seq)\n",
        "Distill **entire output sequences** instead of token-wise targets.  \n",
        "Simplifies decoding in NMT and often removes the need for beam search.\n",
        "\n",
        "---\n",
        "\n",
        "### 7. Self-Distillation / Online & Mutual\n",
        "The model teaches itself:\n",
        "- *Born-Again Networks*: each generation serves as a teacher for the next.\n",
        "- *Deep Mutual Learning*: multiple students learn cooperatively.\n",
        "\n",
        "---\n",
        "\n",
        "### 8. Data-Free / Privacy-Preserving KD\n",
        "Distill knowledge **without access to original data**, using synthetic samples or inversion (e.g., *Zero-Shot KD*, *DeepInversion*).  \n",
        "Useful for **federated** or **privacy-sensitive** contexts.\n",
        "\n",
        "---\n",
        "\n",
        "### 9. Diffusion-Model Distillation\n",
        "Compress diffusion sampling or enable controllable generation:\n",
        "- **Progressive Distillation:** halve steps repeatedly (e.g., 8192 → 4).\n",
        "- **Consistency Models:** single-step generation distilled from diffusion backbones.\n",
        "- **Score-Distillation Sampling (SDS):**\n",
        "  $$ \\nabla_\\theta L = \\mathbb{E}_x [ \\| s_\\theta(x) - s_T(x) \\|^2 ] $$\n",
        "  where \\( s_T \\) is the teacher score.\n",
        "- **Adversarial Diffusion Distillation (ADD):** combines diffusion and GAN objectives.\n",
        "\n",
        "---\n",
        "\n",
        "### 10. LLM-Focused KD\n",
        "Distillation for **Transformers and LLMs**:\n",
        "- *DistilBERT*, *TinyBERT*, *MiniLM(v2)* — combine logit, hidden-state, and attention-relation objectives.\n",
        "- Achieve up to **90 % reduction** in cost while maintaining strong GLUE performance.\n",
        "\n",
        "---\n",
        "\n",
        "## Applications and Goals\n",
        "\n",
        "1. **Compression & Acceleration:** shrink model size and latency while preserving accuracy.  \n",
        "2. **Better Training Signals:** improve generalization in low-data or noisy-label scenarios.  \n",
        "3. **Heterogeneous / Federated Learning:** share only predictions, not parameters.  \n",
        "4. **Diffusion & 3D:** compress multi-step sampling into one or few steps.\n",
        "\n",
        "---\n",
        "\n",
        "## How KD Works (Conceptual Model)\n",
        "\n",
        "**Soft-target alignment:**\n",
        "$$\n",
        "L = (1 - \\alpha)\\,\\text{CE}(y, p_S)\n",
        "    + \\alpha\\,T^2\\,\\text{KL}\\!\\left( p_T^{(T)} \\parallel p_S^{(T)} \\right)\n",
        "$$\n",
        "- Higher \\( T \\) reveals inter-class structure.\n",
        "- \\( \\alpha \\) balances hard vs. soft supervision.\n",
        "\n",
        "**Feature/attention/relational losses** regularize the internal geometry of representations, enhancing robustness to architecture mismatch.\n",
        "\n",
        "**Information-theoretic view:** maximize \\( I(h_T; h_S) \\).  \n",
        "**Sequence-level view:** approximate teacher decoding distributions to simplify generation.\n",
        "\n",
        "---\n",
        "\n",
        "## Practical Recipes\n",
        "\n",
        "**Baseline Logit KD**\n",
        "$$\n",
        "L = \\text{CE}(y, p_S) + \\lambda\\,T^2\\,\\text{KL}\\!\\left( p_T^{(T)} \\parallel p_S^{(T)} \\right)\n",
        "$$\n",
        "Typical ranges: \\( T \\in [2,8] \\), \\( \\lambda \\in [0.5,2] \\).\n",
        "\n",
        "**Intermediate Guidance (Feature KD):**  \n",
        "Match \\( K \\) layers using MSE or cosine similarity (“Patient KD”).\n",
        "\n",
        "**Attention/Relation Losses:**  \n",
        "Add AT/RKD/CRD terms when architectures differ.\n",
        "\n",
        "**For LLMs:**  \n",
        "Combine LM loss + KD loss + attention/hidden-state alignment at both pre-training and task stages.\n",
        "\n",
        "**For Diffusion Models:**  \n",
        "Use progressive or consistency distillation; SDS for cross-space transfer (e.g., NeRF).\n",
        "\n",
        "**For Federated Settings:**  \n",
        "FedMD: share logits on public data; fit local students to ensemble consensus.\n",
        "\n",
        "---\n",
        "\n",
        "## Tuning Tips & Pitfalls\n",
        "\n",
        "- **Temperature \\(T\\)** and **weight \\(\\lambda\\)** are crucial.  \n",
        "  Too low \\(T\\): little dark knowledge; too high \\(T\\): overly flat distributions.\n",
        "- **Teacher quality vs. student capacity:** small students may underfit; feature-based KD helps.\n",
        "- **Tokenizer mismatch (LLMs):** rely on attention/hidden-state KD.\n",
        "- **Data-free KD:** quality of synthetic data matters; expect some accuracy gap.\n",
        "- **Seq-level KD:** balance with a small hard-label loss to avoid over-imitation.\n",
        "- **Diffusion KD:** ensure correct supervision of score dynamics; beware gradient bias in SDS.\n",
        "\n",
        "---\n",
        "\n",
        "## When to Use KD\n",
        "\n",
        "Use KD when:\n",
        "- You need **smaller/faster/cheaper** models.\n",
        "- You are deploying on **edge devices**.\n",
        "- You want to **consolidate ensembles**.\n",
        "- You are aligning **different architectures or modalities**.\n",
        "- You aim to **accelerate diffusion or generative sampling**.\n",
        "\n",
        "Avoid or combine with alternatives when:\n",
        "- Bottleneck is I/O or CPU rather than FLOPs.\n",
        "- Teacher is not substantially better than student.\n",
        "- Pruning or quantization alone achieves the same gains.\n",
        "\n",
        "---\n",
        "\n",
        "## Minimal Starter KD Recipe\n",
        "\n",
        "**Setup**\n",
        "- Teacher: larger, well-trained model.  \n",
        "- Student: thinner or shallower model from the same family.\n",
        "\n",
        "**Loss**\n",
        "$$\n",
        "L = \\text{CE}(y, s)\n",
        "  + \\lambda\\,T^2\\,\\text{KL}\\!\\left( p_T^{(T)} \\parallel p_S^{(T)} \\right)\n",
        "  + \\alpha \\sum_{k=1}^{K} \\| h_T^{(k)} - h_S^{(k)} \\|_2^2\n",
        "$$\n",
        "\n",
        "**Typical Hyperparameters**\n",
        "- \\( T \\in [2,6] \\)\n",
        "- \\( \\lambda \\in [0.5,2] \\)\n",
        "- \\( \\alpha \\in [0.1,0.5] \\)\n",
        "\n",
        "**Evaluation**\n",
        "- Report accuracy vs. compute (latency, memory, energy).  \n",
        "- For diffusion: report FID / IS vs. number of sampling steps.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "mb_eLEAhhwH1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Foundational and Recent Papers on Knowledge Distillation\n",
        "\n",
        "## Root / Foundational Papers\n",
        "\n",
        "**Distilling the Knowledge in a Neural Network**  \n",
        "*(Hinton, Vinyals & Dean, 2015)*  \n",
        "This seminal work introduced **knowledge distillation (KD)** — a process where a smaller *student* model learns from the softened output probabilities of a larger *teacher* model using a **temperature-scaled softmax**. This allows the student to capture “dark knowledge” about inter-class similarities that the teacher has learned.  \n",
        "Source: *arXiv*\n",
        "\n",
        "---\n",
        "\n",
        "**Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks**  \n",
        "*(Papernot et al., 2015)*  \n",
        "This paper introduced **defensive distillation**, showing that the distillation process can enhance **adversarial robustness** by smoothing the model’s decision surface, thus reducing sensitivity to small input perturbations.  \n",
        "Source: *arXiv*\n",
        "\n",
        "---\n",
        "\n",
        "**On the Efficacy of Knowledge Distillation**  \n",
        "*(Cho & Hariharan, ICCV 2019)*  \n",
        "An empirical study exploring *when and why* distillation succeeds or fails. The results show that teacher-student **capacity mismatch** and **architectural differences** significantly affect distillation quality and generalization.  \n",
        "Source: *openaccess.thecvf.com*\n",
        "\n",
        "---\n",
        "\n",
        "**Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning**  \n",
        "*(Allen-Zhu & Li, 2020)*  \n",
        "A theoretical exploration connecting ensembles, distillation, and self-distillation. The paper provides a formal understanding of how ensemble knowledge can be compressed into a single model and explains the benefits of iterative self-distillation.  \n",
        "Source: *arXiv*\n",
        "\n",
        "---\n",
        "\n",
        "**Like What You Like: Knowledge Distill via Neuron Selectivity Transfer**  \n",
        "*(Mocol, Zhang et al., ~2019)*  \n",
        "Proposes **feature-based distillation** using *neuron selectivity distributions* and **maximum mean discrepancy (MMD)** to align internal feature representations between teacher and student.  \n",
        "Source: *openreview.net*\n",
        "\n",
        "---\n",
        "\n",
        "## Recent / Newer Papers\n",
        "\n",
        "**What Knowledge Gets Distilled in Knowledge Distillation?**  \n",
        "*(NeurIPS 2023)*  \n",
        "Investigates *what specific knowledge* is actually transferred during distillation — analyzing whether the student learns structure, label relationships, or other latent representations beyond mere output matching.  \n",
        "Source: *proceedings.neurips.cc*\n",
        "\n",
        "---\n",
        "\n",
        "**A Comprehensive Survey on Knowledge Distillation**  \n",
        "*(Mansourian et al., 2025)*  \n",
        "A comprehensive modern review of KD covering **transformers, large language models (LLMs), and diffusion models**, categorizing methods and analyzing theoretical and practical advances.  \n",
        "Source: *arXiv*\n",
        "\n",
        "---\n",
        "\n",
        "**Knowledge Distillation Meets Self-Supervision**  \n",
        "*(ECCV 2020)*  \n",
        "Integrates **self-supervised learning (SSL)** signals with distillation, enabling students to benefit from unlabeled data and enhancing generalization, especially under limited or noisy labels.  \n",
        "Source: *ecva.net*\n",
        "\n",
        "---\n",
        "\n",
        "**Knowledge Distillation Meets Open-Set Semi-Supervised Learning**  \n",
        "*(2024)*  \n",
        "Extends KD to **open-set semi-supervised** settings, demonstrating that distillation can guide students in domains with partially labeled or unknown-class data.  \n",
        "Source: *SpringerLink*\n",
        "\n",
        "---\n",
        "\n",
        "**A Coded Knowledge Distillation Framework for Image Classification**  \n",
        "*(2024)*  \n",
        "Introduces a **coded representation** approach for KD, encoding teacher knowledge into compact transferable forms to improve student learning in image classification.  \n",
        "Source: *ScienceDirect*\n",
        "\n",
        "---\n",
        "\n",
        "## Additional Notes & Thematic Insights\n",
        "\n",
        "- **Surveys** such as *Knowledge Distillation: A Survey* (Gou et al., 2020) provide taxonomies of KD types (response-based, feature-based, relation-based) and discuss architecture compatibility.  \n",
        "  Source: *arXiv*\n",
        "\n",
        "- **Broader perspective:** KD extends beyond compression — it is also a framework for **knowledge adaptation**, **domain transfer**, and **cross-modal learning**.\n",
        "\n",
        "- **Forms of transferred knowledge:**\n",
        "  - **Logit-based:** Teacher soft outputs.\n",
        "  - **Feature-based:** Intermediate activations or attention maps.\n",
        "  - **Relation-based:** Structural or pairwise sample relationships.\n",
        "\n",
        "- **Practical considerations:**\n",
        "  - Teacher–student capacity ratio.\n",
        "  - Architecture mismatch.\n",
        "  - Data size and availability.\n",
        "  - Distillation temperature and loss weighting.\n",
        "  - Whether distillation improves **generalization** or merely **memorization**.\n",
        "\n",
        "- **Empirical challenges** noted by *Cho & Hariharan (2019)* emphasize that KD is not universally beneficial — effectiveness depends on *teacher quality*, *data overlap*, and *training dynamics*.\n",
        "\n",
        "---\n",
        "\n",
        "## Core Distillation Equation\n",
        "\n",
        "Let the teacher produce logits \\( z_T \\) and the student produce logits \\( z_S \\). The **softened softmax** with temperature \\( T \\) is:\n",
        "\n",
        "$$\n",
        "p_i^{(T)} = \\frac{\\exp(z_i / T)}{\\sum_j \\exp(z_j / T)}\n",
        "$$\n",
        "\n",
        "The **distillation loss** is typically a weighted combination of the Kullback–Leibler divergence between teacher and student outputs and the standard cross-entropy with ground truth labels:\n",
        "\n",
        "$$\n",
        "L = (1 - \\alpha) \\cdot \\text{CE}(y, p_S) + \\alpha \\cdot T^2 \\cdot \\text{KL}(p_T^{(T)} \\parallel p_S^{(T)})\n",
        "$$\n",
        "\n",
        "where:\n",
        "- \\( \\alpha \\) balances between hard and soft targets,  \n",
        "- \\( T \\) controls the smoothness of distributions,  \n",
        "- \\( p_T^{(T)} \\) and \\( p_S^{(T)} \\) denote teacher and student soft probabilities.\n",
        "\n",
        "This formulation captures the essence of how **dark knowledge** (latent inter-class structure) is transferred from teacher to student.\n"
      ],
      "metadata": {
        "id": "hFP7LGRKiDwj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Related Works Connected to *“Distilling the Knowledge in a Neural Network”* (Hinton, Vinyals & Dean, 2015)\n",
        "\n",
        "| **Category** | **Author(s)** | **Year** | **Title** | **Venue / Source** |\n",
        "|---------------|----------------|-----------|------------|--------------------|\n",
        "| **Origin Paper** | Geoffrey E. Hinton, Oriol Vinyals, Jeff Dean | 2015 | *Distilling the Knowledge in a Neural Network* | arXiv.org |\n",
        "| **Prior Works** | Cristian Bucila, R. Caruana, Alexandru Niculescu-Mizil | 2006 | *Model Compression* | KDD ’06 |\n",
        "|  | Jimmy Ba, R. Caruana | 2013 | *Do Deep Nets Really Need to be Deep?* | NIPS |\n",
        "|  | A. Krizhevsky | 2009 | *Learning Multiple Layers of Features from Tiny Images* | University of Toronto Technical Report |\n",
        "|  | Yann LeCun, L. Bottou, Yoshua Bengio, P. Haffner | 1998 | *Gradient-based Learning Applied to Document Recognition* | Proceedings of the IEEE |\n",
        "|  | Jia Deng, Wei Dong, R. Socher, Li-Jia Li, K. Li, Li Fei-Fei | 2009 | *ImageNet: A Large-Scale Hierarchical Image Database* | CVPR |\n",
        "| **Derivative Works** | Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, C. Gatta, Yoshua Bengio | 2014 | *FitNets: Hints for Thin Deep Nets* | ICLR |\n",
        "|  | Sergey Zagoruyko, N. Komodakis | 2016 | *Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer* | ICLR |\n",
        "|  | Yonglong Tian, Dilip Krishnan, Phillip Isola | 2019 | *Contrastive Representation Distillation* | CVPR |\n",
        "|  | Shan You, Chang Xu, Chao Xu, D. Tao | 2017 | *Learning from Multiple Teacher Networks* | KDD |\n",
        "|  | Guobin Chen, Wongun Choi, Xiang Yu, T. Han, Manmohan Chandraker | 2017 | *Learning Efficient Object Detection Models with Knowledge Distillation* | NIPS |\n",
        "|  | Baoyun Peng, Xiao Jin, Jiaheng Liu, Shunfeng Zhou, Yichao Wu, Yu Liu, Dongsheng Li, Zhaoning Zhang | 2019 | *Correlation Congruence for Knowledge Distillation* | ICCV |\n",
        "|  | Ying Zhang, T. Xiang, Timothy M. Hospedales, Huchuan Lu | 2017 | *Deep Mutual Learning* | CVPR |\n",
        "|  | Jianping Gou, B. Yu, S. Maybank, D. Tao | 2020 | *Knowledge Distillation: A Survey* | IJCV |\n",
        "|  | Sungsoo Ahn, S. Hu, Andreas C. Damianou, Neil D. Lawrence, Zhenwen Dai | 2019 | *Variational Information Distillation for Knowledge Transfer* | CVPR |\n",
        "|  | Byeongho Heo, Minsik Lee, Sangdoo Yun, J. Choi | 2018 | *Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons* | AAAI |\n",
        "|  | Jangho Kim, Seonguk Park, Nojun Kwak | 2018 | *Paraphrasing Complex Network: Network Compression via Factor Transfer* | NIPS |\n",
        "|  | Suraj Srinivas, R. Venkatesh Babu | 2015 | *Data-Free Parameter Pruning for Deep Neural Networks* | BMVC |\n",
        "|  | Seyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa, H. Ghasemzadeh | 2019 | *Improved Knowledge Distillation via Teacher Assistant* | AAAI |\n",
        "|  | Tommaso Furlanello, Zachary Chase Lipton, Michael Tschannen, L. Itti, Anima Anandkumar | 2018 | *Born Again Neural Networks* | ICML |\n",
        "|  | Guodong Xu, Ziwei Liu, Xiaoxiao Li, Chen Change Loy | 2020 | *Knowledge Distillation Meets Self-Supervision* | ECCV |\n",
        "|  | Xu Lan, Xiatian Zhu, S. Gong | 2018 | *Knowledge Distillation by On-the-Fly Native Ensemble* | NIPS |\n",
        "|  | Borui Zhao, Quan Cui, Renjie Song, Yiyu Qiu, Jiajun Liang | 2022 | *Decoupled Knowledge Distillation* | CVPR |\n",
        "|  | Hao Li, Asim Kadav, Igor Durdanovic, H. Samet, H. Graf | 2016 | *Pruning Filters for Efficient ConvNets* | ICLR |\n",
        "|  | Pengguang Chen, Shu Liu, Hengshuang Zhao, Jiaya Jia | 2021 | *Distilling Knowledge via Knowledge Review* | CVPR |\n",
        "|  | Quanquan Li, Sheng Jin, Junjie Yan | 2017 | *Mimicking Very Efficient Network for Object Detection* | CVPR |\n",
        "|  | Lin Wang, Kuk-Jin Yoon | 2020 | *Knowledge Distillation and Student-Teacher Learning for Visual Intelligence: A Review and New Outlooks* | IEEE TPAMI |\n",
        "|  | Xiao Jin, Baoyun Peng, Yichao Wu, Yu Liu, Jiaheng Liu, Ding Liang, Junjie Yan, Xiaolin Hu | 2019 | *Knowledge Distillation via Route Constrained Optimization* | ICCV |\n",
        "|  | Li Liu, Qingle Huang, Sihao Lin, Hongwei Xie, Bing Wang, Xiaojun Chang, Xiao-Xue Liang | 2021 | *Exploring Inter-Channel Correlation for Diversity-Preserved Knowledge Distillation* | CVPR |\n",
        "|  | Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, M. Andreetto, Hartwig Adam | 2017 | *MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications* | arXiv |\n",
        "|  | Tao Wang, Li Yuan, Xiaopeng Zhang, Jiashi Feng | 2019 | *Distilling Object Detectors with Fine-Grained Feature Imitation* | CVPR |\n",
        "|  | Jang Hyun Cho, B. Hariharan | 2019 | *On the Efficacy of Knowledge Distillation* | ICCV |\n",
        "|  | Nikolaos Passalis, A. Tefas | 2018 | *Learning Deep Representations with Probabilistic Knowledge Transfer* | ECCV |\n",
        "|  | Defang Chen, Jian-Ping Mei, Yuan Zhang, Can Wang, Zhe Wang, Yan Feng, Chun Chen | 2020 | *Cross-Layer Distillation with Semantic Calibration* | AAAI |\n",
        "|  | Dmytro Mishkin, Jiri Matas | 2015 | *All You Need Is a Good Init* | arXiv |\n"
      ],
      "metadata": {
        "id": "sVsyhoeuvSIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Related Works of *“Distilling the Knowledge in a Neural Network”* (Hinton, Vinyals & Dean, 2015)\n",
        "\n",
        "| **Category** | **Author(s)** | **Year** | **Title** | **Venue / Source** |\n",
        "|---------------|----------------|-----------|------------|--------------------|\n",
        "| **Origin Paper** | Geoffrey E. Hinton, Oriol Vinyals, Jeff Dean | 2015 | *Distilling the Knowledge in a Neural Network* | arXiv.org |\n",
        "| **Prior Works** | Cristian Bucila, R. Caruana, Alexandru Niculescu-Mizil | 2006 | *Model Compression* | KDD |\n",
        "|  | Jimmy Ba, R. Caruana | 2013 | *Do Deep Nets Really Need to be Deep?* | NIPS |\n",
        "|  | A. Krizhevsky | 2009 | *Learning Multiple Layers of Features from Tiny Images* | University of Toronto Technical Report |\n",
        "|  | Jia Deng, Wei Dong, R. Socher, Li-Jia Li, K. Li, Li Fei-Fei | 2009 | *ImageNet: A Large-Scale Hierarchical Image Database* | CVPR |\n",
        "|  | Yann LeCun, L. Bottou, Yoshua Bengio, P. Haffner | 1998 | *Gradient-Based Learning Applied to Document Recognition* | Proceedings of the IEEE |\n",
        "| **Derivative Works** | Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, C. Gatta, Yoshua Bengio | 2014 | *FitNets: Hints for Thin Deep Nets* | ICLR |\n",
        "|  | Sergey Zagoruyko, N. Komodakis | 2016 | *Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer* | ICLR |\n",
        "|  | Yonglong Tian, Dilip Krishnan, Phillip Isola | 2019 | *Contrastive Representation Distillation* | CVPR |\n",
        "|  | Shan You, Chang Xu, Chao Xu, D. Tao | 2017 | *Learning from Multiple Teacher Networks* | KDD |\n",
        "|  | Guobin Chen, Wongun Choi, Xiang Yu, T. Han, Manmohan Chandraker | 2017 | *Learning Efficient Object Detection Models with Knowledge Distillation* | NIPS |\n",
        "|  | Baoyun Peng, Xiao Jin, Jiaheng Liu, Shunfeng Zhou, Yichao Wu, Yu Liu, Dongsheng Li, Zhaoning Zhang | 2019 | *Correlation Congruence for Knowledge Distillation* | ICCV |\n",
        "|  | Ying Zhang, T. Xiang, Timothy M. Hospedales, Huchuan Lu | 2017 | *Deep Mutual Learning* | CVPR |\n",
        "|  | Jianping Gou, B. Yu, S. Maybank, D. Tao | 2020 | *Knowledge Distillation: A Survey* | IJCV |\n",
        "|  | Sungsoo Ahn, S. Hu, Andreas C. Damianou, Neil D. Lawrence, Zhenwen Dai | 2019 | *Variational Information Distillation for Knowledge Transfer* | CVPR |\n",
        "|  | Byeongho Heo, Minsik Lee, Sangdoo Yun, J. Choi | 2018 | *Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons* | AAAI |\n",
        "|  | Jangho Kim, Seonguk Park, Nojun Kwak | 2018 | *Paraphrasing Complex Network: Network Compression via Factor Transfer* | NIPS |\n",
        "|  | Suraj Srinivas, R. Venkatesh Babu | 2015 | *Data-Free Parameter Pruning for Deep Neural Networks* | BMVC |\n",
        "|  | Seyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa, H. Ghasemzadeh | 2019 | *Improved Knowledge Distillation via Teacher Assistant* | AAAI |\n",
        "|  | Tommaso Furlanello, Zachary Chase Lipton, Michael Tschannen, L. Itti, Anima Anandkumar | 2018 | *Born Again Neural Networks* | ICML |\n",
        "|  | Guodong Xu, Ziwei Liu, Xiaoxiao Li, Chen Change Loy | 2020 | *Knowledge Distillation Meets Self-Supervision* | ECCV |\n",
        "|  | Xu Lan, Xiatian Zhu, S. Gong | 2018 | *Knowledge Distillation by On-the-Fly Native Ensemble* | NIPS |\n",
        "|  | Borui Zhao, Quan Cui, Renjie Song, Yiyu Qiu, Jiajun Liang | 2022 | *Decoupled Knowledge Distillation* | CVPR |\n",
        "|  | Hao Li, Asim Kadav, Igor Durdanovic, H. Samet, H. Graf | 2016 | *Pruning Filters for Efficient ConvNets* | ICLR |\n",
        "|  | Pengguang Chen, Shu Liu, Hengshuang Zhao, Jiaya Jia | 2021 | *Distilling Knowledge via Knowledge Review* | CVPR |\n",
        "|  | Quanquan Li, Sheng Jin, Junjie Yan | 2017 | *Mimicking Very Efficient Network for Object Detection* | CVPR |\n",
        "|  | Lin Wang, Kuk-Jin Yoon | 2020 | *Knowledge Distillation and Student-Teacher Learning for Visual Intelligence: A Review and New Outlooks* | IEEE TPAMI |\n",
        "|  | Xiao Jin, Baoyun Peng, Yichao Wu, Yu Liu, Jiaheng Liu, Ding Liang, Junjie Yan, Xiaolin Hu | 2019 | *Knowledge Distillation via Route Constrained Optimization* | ICCV |\n",
        "|  | Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, M. Andreetto, Hartwig Adam | 2017 | *MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications* | arXiv |\n",
        "|  | Tao Wang, Li Yuan, Xiaopeng Zhang, Jiashi Feng | 2019 | *Distilling Object Detectors with Fine-Grained Feature Imitation* | CVPR |\n",
        "|  | Jang Hyun Cho, B. Hariharan | 2019 | *On the Efficacy of Knowledge Distillation* | ICCV |\n",
        "|  | Nikolaos Passalis, A. Tefas | 2018 | *Learning Deep Representations with Probabilistic Knowledge Transfer* | ECCV |\n",
        "|  | Defang Chen, Jian-Ping Mei, Yuan Zhang, Can Wang, Zhe Wang, Yan Feng, Chun Chen | 2020 | *Cross-Layer Distillation with Semantic Calibration* | AAAI |\n",
        "|  | Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf | 2019 | *DistilBERT: A Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter* | arXiv |\n",
        "|  | Yihui He, Xiangyu Zhang, Jian Sun | 2017 | *Channel Pruning for Accelerating Very Deep Neural Networks* | ICCV |\n",
        "|  | Hao Zhou, J. Álvarez, F. Porikli | 2016 | *Less Is More: Towards Compact CNNs* | ECCV |\n",
        "|  | Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang | 2017 | *Learning Efficient Convolutional Networks through Network Slimming* | ICCV |\n",
        "|  | Li Liu, Qingle Huang, Sihao Lin, Hongwei Xie, Bing Wang, Xiaojun Chang, Xiao-Xue Liang | 2021 | *Exploring Inter-Channel Correlation for Diversity-Preserved Knowledge Distillation* | CVPR |\n",
        "|  | Dmytro Mishkin, Jiri Matas | 2015 | *All You Need Is a Good Init* | arXiv |\n"
      ],
      "metadata": {
        "id": "1L1IYL0mvp3B"
      }
    }
  ]
}